# Image Captioning with Generative AI using BLIP from Hugging Face and Gradio

This project demonstrates how to build an image captioning application using the **BLIP (Bootstrapped Language Image Pretraining)** model from Hugging Face and a simple web interface built with **Gradio**. The application takes an input image and generates a descriptive caption using the power of generative AI.

## Project Overview

- **BLIP Model**: A transformer-based model that generates textual captions based on visual input.
- **Gradio**: A low-code tool to create web-based interfaces for machine learning models.

The primary goal of this project is to provide an accessible, easy-to-use interface for non-technical users to explore AI-driven image captioning.

## Features

- Upload an image to the interface.
- Generate and display a caption describing the image's content.
- Simple and intuitive Gradio interface for easy use.
  
## Getting Started

### Prerequisites

To run this project, you need to have the following installed:

- Python 3.x
- Hugging Face Transformers Library
- Gradio Library
- Torch (PyTorch)

### Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/image-captioning-blip.git
